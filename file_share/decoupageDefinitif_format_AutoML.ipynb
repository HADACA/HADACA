{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage définitif des données au format AutoML\n",
    "\n",
    "pour le format AutoML cf. la page https://github.com/madclam/m2aic2019/blob/master/Starting_Kit_M2info.pdf\n",
    "\n",
    "- extraire les données de Magali\n",
    "- les enrichir avec SMOTE\n",
    "- passer au format AutoML (train, valid, test), en découpant de manière à ce que les classes soient équilibrées à chaque fois\n",
    "\n",
    "### PLAN\n",
    "\n",
    "- 1) chargement des données clean et génération du dataset global (1000 de chaque classe avec SMOTE)\n",
    "- 2) découpage auto_ML global (TOUTES nos données): train, valid, test (800,100,100 pour chaque classe)\n",
    "- 3) découpage sample pour starting_kit dans le TRAIN (!!!) précédent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN, SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données clean\n",
    "\n",
    "on charge les données et on utilise SMOTE pour créer l'ensemble \"global\" de données avec lequel on va travailler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data shape: (20103, 687)\n",
      "df_metadata shape: (685, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dmprocr_ID</th>\n",
       "      <th>indiv</th>\n",
       "      <th>sample</th>\n",
       "      <th>trscr</th>\n",
       "      <th>cnv</th>\n",
       "      <th>meth</th>\n",
       "      <th>gender</th>\n",
       "      <th>days_to_birth</th>\n",
       "      <th>tumor_stage</th>\n",
       "      <th>da</th>\n",
       "      <th>fut</th>\n",
       "      <th>age_diag</th>\n",
       "      <th>days_to_death</th>\n",
       "      <th>tissue_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97-7552-01</td>\n",
       "      <td>97-7552</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>-25578.0</td>\n",
       "      <td>stage ib</td>\n",
       "      <td>alive</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>25578.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44-7671-01</td>\n",
       "      <td>44-7671</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>-23538.0</td>\n",
       "      <td>stage ib</td>\n",
       "      <td>alive</td>\n",
       "      <td>889.0</td>\n",
       "      <td>23538.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86-7953-01</td>\n",
       "      <td>86-7953</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>-25315.0</td>\n",
       "      <td>stage ia</td>\n",
       "      <td>alive</td>\n",
       "      <td>997.0</td>\n",
       "      <td>25315.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L4-A4E5-01</td>\n",
       "      <td>L4-A4E5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>-17680.0</td>\n",
       "      <td>stage i</td>\n",
       "      <td>alive</td>\n",
       "      <td>578.0</td>\n",
       "      <td>17680.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NJ-A4YP-01</td>\n",
       "      <td>NJ-A4YP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>-19106.0</td>\n",
       "      <td>stage ib</td>\n",
       "      <td>alive</td>\n",
       "      <td>50.0</td>\n",
       "      <td>19106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dmprocr_ID    indiv  sample  trscr  cnv  meth  gender  days_to_birth  \\\n",
       "0  97-7552-01  97-7552       1      1    1     1    male       -25578.0   \n",
       "1  44-7671-01  44-7671       1      0    1     1    male       -23538.0   \n",
       "2  86-7953-01  86-7953       1      1    1     1  female       -25315.0   \n",
       "3  L4-A4E5-01  L4-A4E5       1      1    1     1  female       -17680.0   \n",
       "4  NJ-A4YP-01  NJ-A4YP       1      1    1     1    male       -19106.0   \n",
       "\n",
       "  tumor_stage     da     fut  age_diag  days_to_death tissue_status  \n",
       "0    stage ib  alive  1932.0   25578.0            NaN         patho  \n",
       "1    stage ib  alive   889.0   23538.0            NaN         patho  \n",
       "2    stage ia  alive   997.0   25315.0            NaN         patho  \n",
       "3     stage i  alive   578.0   17680.0            NaN         patho  \n",
       "4    stage ib  alive    50.0   19106.0            NaN         patho  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement des données, clean_data étant les données nettoyées des NaN trop nombreux\n",
    "df_data = pd.read_csv(\"data/clean_data.csv\")\n",
    "print(\"df_data shape:\", df_data.shape)\n",
    "df_metadata = pd.read_csv(\"data/metadata.csv\")\n",
    "print(\"df_metadata shape:\", df_metadata.shape)\n",
    "df_metadata[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(685, 20103)\n",
      "labelsBinary : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "valuesBinary : Index(['patho', 'normal'], dtype='object')\n",
      "labelsStages : [ 0  0  1  2  0  3  1  4  0  1  0  3  0  5  0  0  5  1  0  6  4  1  1 -1\n",
      " -1  0  1  1  1  0  7 -1  7  0  0  1  4  5  0  0  8  5  6  9  5  7  6 -1\n",
      "  5  3  1  1  1  5  7  5  3 -1 -1  0  5  0  4  7  0  0 -1  1 -1  1  5 -1\n",
      "  5  5  0  1  5 -1  0  3  1  7  3  5  0  5  3  5  0 -1  5  3  0  1  1  0\n",
      " -1  3  7  0  1 -1  5  2  0  5  4  1  0  9  0  3  0  1  0  3  5 -1 -1  5\n",
      " -1  3  1  3  5  9  1  4  0  1  1 -1  5  0  5  0  3  1  1 -1 -1  1  0  0\n",
      "  7  3  1  0  0  3  5  1  1  0  0  0  0  1  3  1  3  1  3  3  1  0  1  0\n",
      "  9 -1 -1  3  0  5  1  7  1  1  1  4  7  3  0  3  5  5  9  1  0  7 -1  1\n",
      "  1  0  3  7  6  1  5  5  5  5  3  0  0  1  1  7  1  1 -1  0  1  0  7  7\n",
      "  0  0  3  0  1  5  1  1  7  0  0  7  1  1  1  1  7  9  7  0  1 -1 -1  0\n",
      "  6  1  0  5  7 -1  5  3  5  3  3  0  1  5  0  5  0  0  3  7  3  5  1  0\n",
      "  3  1  1  4  1  5  0  3  4  1  1  3  1  1  3  0  0  1  1  7  0  5  1  1\n",
      "  5  5  1  1 -1  1  5  0  1  1  3  5  4 -1  3  5 -1  4  7  3  3  0  5  7\n",
      "  0  1  1  1 -1  0  3 -1  3 -1 -1  0  0  1  5  3  1  1  4  3  0  7  0  7\n",
      "  7  3  0 -1  4  1  9  0  3  5  0  1  0  7  0 -1  1 -1  1  0  1  5 -1  0\n",
      "  1  0  1  0  3  1  7  5  1 -1  0  0 -1  1  3  4  0  1  0  3  0  1  6 -1\n",
      "  5  0  1  1  0 -1 -1  3  1  7  4  1  4 -1  3  3  1  2 -1  7  7  5  0  0\n",
      "  1  3  2  3  7  4  5  3  0  7  5 -1  3  0  1  1  1  1  7  3  7  9  1  9\n",
      " -1 -1  5  3  5  5  3  1  1  5  0  1  0  7 -1  4  0  0  3  3  1  0  4  1\n",
      "  0  1  5  1  0  7  0  3  1  1 -1  7 -1  3  0  5  0  7  2  0  1  0  1  1\n",
      "  0  7  5  0  0  1  0  5  0  0  0  0  7  3  3  7  0  1  3  0  7  4 -1  3\n",
      "  1  5  3  1  3  3  3  4  7  1  0  4  3  0  5  6  0  0  0  1  0  0  4  0\n",
      "  7 -1  3 -1  0  0  3  5  5  1  1  1  3  5  1  5  6 -1  3  5  3  0  0  5\n",
      "  1  1  1  7  5  7  0  5  0  3  1  0  0  0  3 -1  1  1  0  1  4 -1  1  0\n",
      "  1  5  0  1  0  9  1  5  5  5  1  5  4 -1  0  7  3  5 -1  0 -1  0 -1  4\n",
      "  0  1  1  4  6  1  0  0  5  0  4  0  1  5  9  9 -1  3  1  0  0  3  5  0\n",
      "  4  0  3  4  3  0  1  3  0  5  1  0  0  6  0  1  0  1  0  1  1  1  5  0\n",
      "  1  1  5  3  0  1  7  0  0  5  5  5  1  3  1  0  7  7  0  0  7  0  0  0\n",
      "  3  4  3  0  5  3  1  0  9  3  9  3  9]\n",
      "valuesStages : Index(['stage ib', 'stage ia', 'stage i', 'stage iib', 'stage iv',\n",
      "       'stage iiia', 'not reported', 'stage iia', 'stage ii', 'stage iiib'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Convertir les données en ndarray et supprimer les colonnes inutiles\n",
    "D = df_data.loc[:, ~df_data.columns.str.contains('^Unnamed')].values\n",
    "D = D.T\n",
    "\n",
    "print(type(D))\n",
    "print(D.shape)\n",
    "\n",
    "# Générer les labels en fonction d'une colonne choisie\n",
    "status = pd.Series(df_metadata[\"tissue_status\"].values)\n",
    "stage = pd.Series(df_metadata[\"tumor_stage\"].values)\n",
    "\n",
    "labelsBinary, valuesBinary = pd.factorize(status)\n",
    "labelsStages, valuesStages = pd.factorize(stage)\n",
    "\n",
    "yBinary = labelsBinary\n",
    "yStage = labelsStages\n",
    "\n",
    "print(\"labelsBinary :\", labelsBinary)\n",
    "print(\"valuesBinary :\", valuesBinary)\n",
    "\n",
    "print(\"labelsStages :\", labelsStages)\n",
    "print(\"valuesStages :\", valuesStages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(685, 1000)\n"
     ]
    }
   ],
   "source": [
    "# IL FAUT ARRIVER À 300 MB après avoir agrandi le nombre d'observations\n",
    "# ===>>> couper les features\n",
    "# selection des k best features grâce au test chi2\n",
    "chi2_selector = SelectKBest(chi2, k=1000)\n",
    "D = chi2_selector.fit_transform(D, labelsBinary)\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 141, 1: 131, 5: 68, 3: 63, -1: 49, 7: 42, 4: 26, 9: 13, 6: 9, 2: 5, 8: 1})\n",
      "(547, 1000)\n",
      "(1000,)\n",
      "(548, 1000)\n",
      "[ 0  0  7  1  7  0  5  3  4 -1  0  0  5  0  7  0  6 -1  1  3  0  0  1  1\n",
      "  3 -1  1  5  1  5  0  3  0  4  7  7  3  3  0  0  1  0  0  0  4 -1 -1 -1\n",
      "  1  0  1  0 -1  0  9  6  5  0  0  0 -1  3  1  7  0  0  5  0  1  1  1  1\n",
      " -1  5  4  3  4  3 -1  0  7  0  5  1  1  6  1  9  1  0  7  5  1  1  0  5\n",
      "  7  0  5  0  3  0  3  0  0  6  7  0  1  1  5  3  7  0  1  5  5  1  1  7\n",
      "  0  1  0 -1 -1  0  1  0  0 -1  9  0 -1  0  3  1  0  7  5  1  3 -1  1  1\n",
      "  7  0  1  1  0  1  1  6  5  7  9  1  1  0  7  3  1  1  0  7  4  4  3  5\n",
      "  5  3  5 -1 -1  0  7  3  0  1  3  6  0  9  0  6  0  4  0  3 -1  1  7 -1\n",
      "  3  1  4  0  1  4  1  1  1  5  0 -1  9  1  0  1  1 -1  5  5  0 -1  3  4\n",
      "  0  1  7 -1  1  1  1  1  7  3  7  0  0  4  5  5  0  3  2  5  7 -1  1  7\n",
      "  0  5  1  0  1  1  0  1  3  1  5  1 -1  0  3  0  7  1  5  3  9 -1  0  5\n",
      "  1  1  5  1  9  9  3  3  0  0  1  1 -1 -1  5  1  1  1  0  0  0  2  0  0\n",
      "  6  0  3  0  5  5  5  1  7  1  7  7  7  5 -1  0  1  4 -1  5  5  4  5  1\n",
      "  0  3  5  5  1  1  1  1  5  5 -1  1  1 -1  0  0  0  4  1  0  3  5 -1  0\n",
      "  1  3  1  0 -1  0  5  0  1  0  3  1  9  3  0 -1  0  0  4  3  4  3  0  0\n",
      "  0  1  0  1  1  3  3  3  0  5  3  1  5  0  3  1  1 -1  5  0  0  3  0  4\n",
      "  7  1  1  3  4  5  0  0  7  5  1  9  1  0  2  4 -1  3  1  0  5  7  1  0\n",
      "  0  1  5  3  0  1  5  0  1  0  1  3 -1  5  1  1  0  6  0  1 -1  1  1 -1\n",
      "  5  0  0  0  3  7 -1  0 -1 -1  7  5  5  1  0  1  0  0  5  5  3  3  1  4\n",
      "  5  0  7  0  0  0  0  7  3  1  0  1  3  0  0  7  1  9  5  5  1  7  2  1\n",
      "  5  0 -1  0  1  3  1  1  0  3  3  4  7  3 -1  0  5  1  5  1  0  1  1  1\n",
      "  0  0  7  1  5  3  1  7  0  3 -1  2  0  1  1  1  1  3  1  1  0  4  0  0\n",
      "  3  3  1  0  0 -1  5  5  4  7 -1  3  9  4 -1  4  0  3  5]\n",
      "[ 0  0  7  1  7  0  5  3  4 -1  0  0  5  0  7  0  6 -1  1  3  0  0  1  1\n",
      "  3 -1  1  5  1  5  0  3  0  4  7  7  3  3  0  0  1  0  0  0  4 -1 -1 -1\n",
      "  1  0  1  0 -1  0  9  6  5  0  0  0 -1  3  1  7  0  0  5  0  1  1  1  1\n",
      " -1  5  4  3  4  3 -1  0  7  0  5  1  1  6  1  9  1  0  7  5  1  1  0  5\n",
      "  7  0  5  0  3  0  3  0  0  6  7  0  1  1  5  3  7  0  1  5  5  1  1  7\n",
      "  0  1  0 -1 -1  0  1  0  0 -1  9  0 -1  0  3  1  0  7  5  1  3 -1  1  1\n",
      "  7  0  1  1  0  1  1  6  5  7  9  1  1  0  7  3  1  1  0  7  4  4  3  5\n",
      "  5  3  5 -1 -1  0  7  3  0  1  3  6  0  9  0  6  0  4  0  3 -1  1  7 -1\n",
      "  3  1  4  0  1  4  1  1  1  5  0 -1  9  1  0  1  1 -1  5  5  0 -1  3  4\n",
      "  0  1  7 -1  1  1  1  1  7  3  7  0  0  4  5  5  0  3  2  5  7 -1  1  7\n",
      "  0  5  1  0  1  1  0  1  3  1  5  1 -1  0  3  0  7  1  5  3  9 -1  0  5\n",
      "  1  1  5  1  9  9  3  3  0  0  1  1 -1 -1  5  1  1  1  0  0  0  2  0  0\n",
      "  6  0  3  0  5  5  5  1  7  1  7  7  7  5 -1  0  1  4 -1  5  5  4  5  1\n",
      "  0  3  5  5  1  1  1  1  5  5 -1  1  1 -1  0  0  0  4  1  0  3  5 -1  0\n",
      "  1  3  1  0 -1  0  5  0  1  0  3  1  9  3  0 -1  0  0  4  3  4  3  0  0\n",
      "  0  1  0  1  1  3  3  3  0  5  3  1  5  0  3  1  1 -1  5  0  0  3  0  4\n",
      "  7  1  1  3  4  5  0  0  7  5  1  9  1  0  2  4 -1  3  1  0  5  7  1  0\n",
      "  0  1  5  3  0  1  5  0  1  0  1  3 -1  5  1  1  0  6  0  1 -1  1  1 -1\n",
      "  5  0  0  0  3  7 -1  0 -1 -1  7  5  5  1  0  1  0  0  5  5  3  3  1  4\n",
      "  5  0  7  0  0  0  0  7  3  1  0  1  3  0  0  7  1  9  5  5  1  7  2  1\n",
      "  5  0 -1  0  1  3  1  1  0  3  3  4  7  3 -1  0  5  1  5  1  0  1  1  1\n",
      "  0  0  7  1  5  3  1  7  0  3 -1  2  0  1  1  1  1  3  1  1  0  4  0  0\n",
      "  3  3  1  0  0 -1  5  5  4  7 -1  3  9  4 -1  4  0  3  5  2]\n"
     ]
    }
   ],
   "source": [
    "# 6 occurences nécessaire pour Smote ou Adasyn, \n",
    "# on enlève la classe 8 et on duplique une occurence de 2 pour passer à 6 (min nécessaire à SMOTE) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(D, yStage, test_size=0.2, random_state=42)\n",
    "# stratify=y, mais une classe avec 1 occurences,on vire cette classe?\n",
    "\n",
    "recounted = Counter(y_train)\n",
    "print(recounted)\n",
    "\n",
    "X_train_sans_8 = X_train[np.where(y_train!=8)]\n",
    "X_2 = X_train[np.where(y_train==2)][0]\n",
    "print(X_train_sans_8.shape)\n",
    "print(X_2.shape)\n",
    "\n",
    "# dédoublement d'un exemple de la classe 2\n",
    "X_train_sans_8_double_2 = np.vstack([X_train_sans_8,X_2])\n",
    "print(X_train_sans_8_double_2.shape)\n",
    "\n",
    "# dédoublement d'un label de la classe 2\n",
    "y_train_sans_8 = y_train[np.where(y_train!=8)]\n",
    "print(y_train_sans_8)\n",
    "y_train_sans_8 = np.append(y_train_sans_8, 2)\n",
    "print(y_train_sans_8)\n",
    "\n",
    "# retrait des instances potentielles de la classe 8 trop petite dans le test\n",
    "X_test = X_test[np.where(y_test!=8)]\n",
    "y_test = y_test[np.where(y_test!=8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 0 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 1 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 5 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 3 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class -1 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 7 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 4 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 9 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 6 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 2 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "(10000, 1000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_sans_8_double_2\n",
    "y_train = y_train_sans_8\n",
    "\n",
    "# nombre d'occurences désirées par classe \n",
    "# # on a enlevé la classe 8 car qu'une occurence \n",
    "# impossible d'appliquer SMOTE ou ADASYN et génération d'une population à partir\n",
    "# d'un seul exemple est absurde\n",
    "\n",
    "dict= {0: 1000, 1: 1000, 5: 1000, 3: 1000, -1: 1000, 7: 1000, 4: 1000, 9: 1000, 6: 1000, 2: 1000}  \n",
    "smote = SMOTE(random_state=42, sampling_strategy=dict)\n",
    "\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "y_resampled = np.array([y_1 if y_1 != -1 else 8 for y_1 in y_resampled])\n",
    "print(set(y_resampled))\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage auto_ML global (TOUTES nos données): train, valid, test\n",
    "\n",
    "chaque ensemble (train, valid, test) doit être équilibré\n",
    "\n",
    "La dernière colonne contient les labels pour X_and_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1001)\n"
     ]
    }
   ],
   "source": [
    "# on récupère un data_frame de chaque classe, dans lesquelles on va piocher pour redéfinir chaque ensemble\n",
    "\n",
    "X_resampled_df = pd.DataFrame(X_resampled)\n",
    "y_resampled_df = pd.DataFrame(y_resampled)\n",
    "y_resampled_df = y_resampled_df.rename(columns={0: 'label'})\n",
    "\n",
    "\n",
    "# La dernière colonne contient les labels\n",
    "X_and_y = pd.concat([X_resampled_df, y_resampled_df], axis=1, sort=False)\n",
    "#X_and_y.head(n=2)\n",
    "print(X_and_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_and_y['label'].values) # vérifie la disparition de la classe -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition d'un dataframe par classe pour découper par classe\n",
    "# et obtenir des train/valid/test équilibrés\n",
    "\n",
    "X_and_y_class0 = X_and_y[X_and_y[\"label\"] == 0]\n",
    "X_and_y_class1 = X_and_y[X_and_y[\"label\"] == 1]\n",
    "X_and_y_class2 = X_and_y[X_and_y[\"label\"] == 2]\n",
    "X_and_y_class3 = X_and_y[X_and_y[\"label\"] == 3]\n",
    "X_and_y_class4 = X_and_y[X_and_y[\"label\"] == 4]\n",
    "X_and_y_class5 = X_and_y[X_and_y[\"label\"] == 5]\n",
    "X_and_y_class6 = X_and_y[X_and_y[\"label\"] == 6]\n",
    "X_and_y_class7 = X_and_y[X_and_y[\"label\"] == 7]\n",
    "X_and_y_class8 = X_and_y[X_and_y[\"label\"] == 8]\n",
    "X_and_y_class9 = X_and_y[X_and_y[\"label\"] == 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN global de taille 800 (dans lequel on prendra tous les sets du starting kit)\n",
    "\n",
    "La dernière colonne contient les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1001)\n",
      "(8000, 1001)\n"
     ]
    }
   ],
   "source": [
    "X_and_y_class0_train = X_and_y_class0[:800]\n",
    "print(X_and_y_class0_train.shape)\n",
    "\n",
    "X_and_y_class1_train = X_and_y_class1[:800]\n",
    "X_and_y_class2_train = X_and_y_class2[:800]\n",
    "X_and_y_class3_train = X_and_y_class3[:800]\n",
    "X_and_y_class4_train = X_and_y_class4[:800]\n",
    "X_and_y_class5_train = X_and_y_class5[:800]\n",
    "X_and_y_class6_train = X_and_y_class6[:800]\n",
    "X_and_y_class7_train = X_and_y_class7[:800]\n",
    "X_and_y_class8_train = X_and_y_class8[:800]\n",
    "X_and_y_class9_train = X_and_y_class9[:800]\n",
    "\n",
    "# TRAIN global par concaténations des 800 premiers de chaque classe pour obtenir un train équilibré\n",
    "X_and_y_train = pd.concat([X_and_y_class0_train,X_and_y_class1_train, X_and_y_class2_train,X_and_y_class3_train,X_and_y_class4_train,X_and_y_class5_train,X_and_y_class6_train,X_and_y_class7_train,X_and_y_class8_train,X_and_y_class9_train], axis=0, sort=False)\n",
    "print(X_and_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALID global de taille 100 (dans lequel on ne prendra RIEN pour le starting kit)\n",
    "\n",
    "La dernière colonne contient les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1001)\n"
     ]
    }
   ],
   "source": [
    "X_and_y_class0_valid = X_and_y_class0[800:900]\n",
    "X_and_y_class1_valid = X_and_y_class1[800:900]\n",
    "X_and_y_class2_valid = X_and_y_class2[800:900]\n",
    "X_and_y_class3_valid = X_and_y_class3[800:900]\n",
    "X_and_y_class4_valid = X_and_y_class4[800:900]\n",
    "X_and_y_class5_valid = X_and_y_class5[800:900]\n",
    "X_and_y_class6_valid = X_and_y_class6[800:900]\n",
    "X_and_y_class7_valid = X_and_y_class7[800:900]\n",
    "X_and_y_class8_valid = X_and_y_class8[800:900]\n",
    "X_and_y_class9_valid = X_and_y_class9[800:900]\n",
    "\n",
    "# TRAIN global par concaténations des 800 premiers de chaque classe pour obtenir un train équilibré\n",
    "X_and_y_valid = pd.concat([X_and_y_class0_valid,X_and_y_class1_valid, X_and_y_class2_valid,X_and_y_class3_valid,X_and_y_class4_valid,X_and_y_class5_valid,X_and_y_class6_valid,X_and_y_class7_valid,X_and_y_class8_valid,X_and_y_class9_valid], axis=0, sort=False)\n",
    "print(X_and_y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST global de taille 100 (dans lequel on ne prendra RIEN pour le starting kit)\n",
    "\n",
    "La dernière colonne contient les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1001)\n"
     ]
    }
   ],
   "source": [
    "X_and_y_class0_test = X_and_y_class0[800:900]\n",
    "X_and_y_class1_test = X_and_y_class1[800:900]\n",
    "X_and_y_class2_test = X_and_y_class2[800:900]\n",
    "X_and_y_class3_test = X_and_y_class3[800:900]\n",
    "X_and_y_class4_test = X_and_y_class4[800:900]\n",
    "X_and_y_class5_test = X_and_y_class5[800:900]\n",
    "X_and_y_class6_test = X_and_y_class6[800:900]\n",
    "X_and_y_class7_test = X_and_y_class7[800:900]\n",
    "X_and_y_class8_test = X_and_y_class8[800:900]\n",
    "X_and_y_class9_test = X_and_y_class9[800:900]\n",
    "\n",
    "# TRAIN global par concaténations des 800 premiers de chaque classe pour obtenir un train équilibré\n",
    "X_and_y_test = pd.concat([X_and_y_class0_test,X_and_y_class1_test, X_and_y_class2_test,X_and_y_class3_test,X_and_y_class4_test,X_and_y_class5_test,X_and_y_class6_test,X_and_y_class7_test,X_and_y_class8_test,X_and_y_class9_test], axis=0, sort=False)\n",
    "print(X_and_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage sample pour starting_kit dans le train global défini dans les cellules précédentes (sample train/valid/test provenant TOUS du train global !!!)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Générer des sets *équilibrés* pour le starting kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 différents de chaque classe dans chacun des subsets du starting kit\n",
    "\n",
    "# TRAIN pour starting_kit\n",
    "X_and_y_class0_trainKit = X_and_y_class0[:10]\n",
    "X_and_y_class1_trainKit = X_and_y_class1[:10]\n",
    "X_and_y_class2_trainKit = X_and_y_class2[:10]\n",
    "X_and_y_class3_trainKit = X_and_y_class3[:10]\n",
    "X_and_y_class4_trainKit = X_and_y_class4[:10]\n",
    "X_and_y_class5_trainKit = X_and_y_class5[:10]\n",
    "X_and_y_class6_trainKit = X_and_y_class6[:10]\n",
    "X_and_y_class7_trainKit = X_and_y_class7[:10]\n",
    "X_and_y_class8_trainKit = X_and_y_class8[:10]\n",
    "X_and_y_class9_trainKit = X_and_y_class9[:10]\n",
    "\n",
    "X_and_y_trainKit = pd.concat([X_and_y_class0_trainKit,X_and_y_class1_trainKit,\\\n",
    "                              X_and_y_class2_trainKit,X_and_y_class3_trainKit,\\\n",
    "                              X_and_y_class4_trainKit,X_and_y_class5_trainKit,\\\n",
    "                              X_and_y_class6_trainKit,X_and_y_class7_trainKit,\\\n",
    "                              X_and_y_class8_trainKit,X_and_y_class9_trainKit,], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST pour starting_kit\n",
    "X_and_y_class0_testKit = X_and_y_class0[10:20]\n",
    "X_and_y_class1_testKit = X_and_y_class1[10:20]\n",
    "X_and_y_class2_testKit = X_and_y_class2[10:20]\n",
    "X_and_y_class3_testKit = X_and_y_class3[10:20]\n",
    "X_and_y_class4_testKit = X_and_y_class4[10:20]\n",
    "X_and_y_class5_testKit = X_and_y_class5[10:20]\n",
    "X_and_y_class6_testKit = X_and_y_class6[10:20]\n",
    "X_and_y_class7_testKit = X_and_y_class7[10:20]\n",
    "X_and_y_class8_testKit = X_and_y_class8[10:20]\n",
    "X_and_y_class9_testKit = X_and_y_class9[10:20]\n",
    "\n",
    "X_and_y_testKit = pd.concat([X_and_y_class0_testKit,X_and_y_class1_testKit,\\\n",
    "                              X_and_y_class2_testKit,X_and_y_class3_testKit,\\\n",
    "                              X_and_y_class4_testKit,X_and_y_class5_testKit,\\\n",
    "                              X_and_y_class6_testKit,X_and_y_class7_testKit,\\\n",
    "                              X_and_y_class8_testKit,X_and_y_class9_testKit,], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID pour starting_kit\n",
    "X_and_y_class0_validKit = X_and_y_class0[20:30]\n",
    "X_and_y_class1_validKit = X_and_y_class1[20:30]\n",
    "X_and_y_class2_validKit = X_and_y_class2[20:30]\n",
    "X_and_y_class3_validKit = X_and_y_class3[20:30]\n",
    "X_and_y_class4_validKit = X_and_y_class4[20:30]\n",
    "X_and_y_class5_validKit = X_and_y_class5[20:30]\n",
    "X_and_y_class6_validKit = X_and_y_class6[20:30]\n",
    "X_and_y_class7_validKit = X_and_y_class7[20:30]\n",
    "X_and_y_class8_validKit = X_and_y_class8[20:30]\n",
    "X_and_y_class9_validKit = X_and_y_class9[20:30]\n",
    "\n",
    "X_and_y_validKit = pd.concat([X_and_y_class0_validKit,X_and_y_class1_validKit,\\\n",
    "                              X_and_y_class2_validKit,X_and_y_class3_validKit,\\\n",
    "                              X_and_y_class4_validKit,X_and_y_class5_validKit,\\\n",
    "                              X_and_y_class6_validKit,X_and_y_class7_validKit,\\\n",
    "                              X_and_y_class8_validKit,X_and_y_class9_validKit,], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Il faut re-séparer les features des labels (contenus dans la dernière colonne des X_and_y_...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_startingKit = X_and_y_train.iloc[:,:-1] # pour l'écriture du .name\n",
    "\n",
    "X_trainKit = X_and_y_trainKit.iloc[:,:-1]\n",
    "X_testKit = X_and_y_testKit.iloc[:,:-1]\n",
    "X_validKit = X_and_y_validKit.iloc[:,:-1]\n",
    "\n",
    "Y_trainKit = X_and_y_trainKit.iloc[:,-1]\n",
    "Y_testKit = X_and_y_testKit.iloc[:,-1]\n",
    "Y_validKit = X_and_y_validKit.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.702318</td>\n",
       "      <td>0.724066</td>\n",
       "      <td>0.696525</td>\n",
       "      <td>0.434269</td>\n",
       "      <td>0.142745</td>\n",
       "      <td>0.771418</td>\n",
       "      <td>0.770929</td>\n",
       "      <td>0.348270</td>\n",
       "      <td>0.606192</td>\n",
       "      <td>0.518180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579567</td>\n",
       "      <td>0.543456</td>\n",
       "      <td>0.383728</td>\n",
       "      <td>0.361674</td>\n",
       "      <td>0.624254</td>\n",
       "      <td>0.409846</td>\n",
       "      <td>0.705805</td>\n",
       "      <td>0.550353</td>\n",
       "      <td>0.352172</td>\n",
       "      <td>0.375828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.062884</td>\n",
       "      <td>0.691114</td>\n",
       "      <td>0.732033</td>\n",
       "      <td>0.467048</td>\n",
       "      <td>0.079799</td>\n",
       "      <td>0.260023</td>\n",
       "      <td>0.614715</td>\n",
       "      <td>0.176157</td>\n",
       "      <td>0.183381</td>\n",
       "      <td>0.022678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038335</td>\n",
       "      <td>0.116865</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.514503</td>\n",
       "      <td>0.632246</td>\n",
       "      <td>0.050003</td>\n",
       "      <td>0.428214</td>\n",
       "      <td>0.485079</td>\n",
       "      <td>0.195186</td>\n",
       "      <td>0.717035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.702318  0.724066  0.696525  0.434269  0.142745  0.771418  0.770929   \n",
       "1  0.062884  0.691114  0.732033  0.467048  0.079799  0.260023  0.614715   \n",
       "\n",
       "        7         8         9      ...          990       991       992  \\\n",
       "0  0.348270  0.606192  0.518180    ...     0.579567  0.543456  0.383728   \n",
       "1  0.176157  0.183381  0.022678    ...     0.038335  0.116865  0.040400   \n",
       "\n",
       "        993       994       995       996       997       998       999  \n",
       "0  0.361674  0.624254  0.409846  0.705805  0.550353  0.352172  0.375828  \n",
       "1  0.514503  0.632246  0.050003  0.428214  0.485079  0.195186  0.717035  \n",
       "\n",
       "[2 rows x 1000 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainKit.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_trainKit.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) puis écrire dans les fichiers AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../starting_kit/sample_data/hadaca_feat.name', 'w') as f:\n",
    "    for i in range(X_trainKit.values.shape[1]):\n",
    "        f.write('methyl_{}\\n'.format(i))\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_train.data', 'w') as f:\n",
    "    for x in X_trainKit.values:\n",
    "        for feat in x:\n",
    "            f.write('{} '.format(np.float64(feat)))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_test.data', 'w') as f:\n",
    "    for x in X_testKit.values:\n",
    "        for feat in x:\n",
    "            f.write('{} '.format(np.float64(feat)))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_valid.data', 'w') as f:\n",
    "    for x in X_validKit.values:\n",
    "        for feat in x:\n",
    "            f.write('{} '.format(np.float64(feat)))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_train.solution', 'w') as f:\n",
    "    for x in Y_trainKit.values:\n",
    "        f.write('{}'.format(x))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_test.solution', 'w') as f:\n",
    "    for x in Y_testKit.values:\n",
    "        f.write('{}'.format(x))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_valid.solution', 'w') as f:\n",
    "    for x in Y_validKit.values:\n",
    "        f.write('{}'.format(x))\n",
    "\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
