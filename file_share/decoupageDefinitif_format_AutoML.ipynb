{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage définitif des données au format AutoML\n",
    "\n",
    "pour le format AutoML cf. la page https://github.com/madclam/m2aic2019/blob/master/Starting_Kit_M2info.pdf\n",
    "\n",
    "- extraire les données de Magali\n",
    "- les enrichir avec SMOTE\n",
    "- passer au format AutoML (train, valid, test), en découpant de manière à ce que les classes soient équilibrées à chaque fois\n",
    "\n",
    "### PLAN\n",
    "\n",
    "- 1) chargement des données clean et génération du dataset global (1000 de chaque classe avec SMOTE)\n",
    "- 2) découpage auto_ML global (TOUTES nos données): train, valid, test (800,100,100 pour chaque classe)\n",
    "- 3) découpage sample pour starting_kit dans le TRAIN (!!!) précédent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_squared_error, log_loss\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données clean\n",
    "\n",
    "on charge les données et on utilise SMOTE pour créer l'ensemble \"global\" de données avec lequel on va travailler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_data shape: (20103, 687)\n",
      "df_metadata shape: (685, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dmprocr_ID</th>\n",
       "      <th>indiv</th>\n",
       "      <th>sample</th>\n",
       "      <th>trscr</th>\n",
       "      <th>cnv</th>\n",
       "      <th>meth</th>\n",
       "      <th>gender</th>\n",
       "      <th>days_to_birth</th>\n",
       "      <th>tumor_stage</th>\n",
       "      <th>da</th>\n",
       "      <th>fut</th>\n",
       "      <th>age_diag</th>\n",
       "      <th>days_to_death</th>\n",
       "      <th>tissue_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>97-7552-01</td>\n",
       "      <td>97-7552</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>-25578.0</td>\n",
       "      <td>stage ib</td>\n",
       "      <td>alive</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>25578.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44-7671-01</td>\n",
       "      <td>44-7671</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>-23538.0</td>\n",
       "      <td>stage ib</td>\n",
       "      <td>alive</td>\n",
       "      <td>889.0</td>\n",
       "      <td>23538.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86-7953-01</td>\n",
       "      <td>86-7953</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>-25315.0</td>\n",
       "      <td>stage ia</td>\n",
       "      <td>alive</td>\n",
       "      <td>997.0</td>\n",
       "      <td>25315.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L4-A4E5-01</td>\n",
       "      <td>L4-A4E5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>-17680.0</td>\n",
       "      <td>stage i</td>\n",
       "      <td>alive</td>\n",
       "      <td>578.0</td>\n",
       "      <td>17680.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NJ-A4YP-01</td>\n",
       "      <td>NJ-A4YP</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>-19106.0</td>\n",
       "      <td>stage ib</td>\n",
       "      <td>alive</td>\n",
       "      <td>50.0</td>\n",
       "      <td>19106.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>patho</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dmprocr_ID    indiv  sample  trscr  cnv  meth  gender  days_to_birth  \\\n",
       "0  97-7552-01  97-7552       1      1    1     1    male       -25578.0   \n",
       "1  44-7671-01  44-7671       1      0    1     1    male       -23538.0   \n",
       "2  86-7953-01  86-7953       1      1    1     1  female       -25315.0   \n",
       "3  L4-A4E5-01  L4-A4E5       1      1    1     1  female       -17680.0   \n",
       "4  NJ-A4YP-01  NJ-A4YP       1      1    1     1    male       -19106.0   \n",
       "\n",
       "  tumor_stage     da     fut  age_diag  days_to_death tissue_status  \n",
       "0    stage ib  alive  1932.0   25578.0            NaN         patho  \n",
       "1    stage ib  alive   889.0   23538.0            NaN         patho  \n",
       "2    stage ia  alive   997.0   25315.0            NaN         patho  \n",
       "3     stage i  alive   578.0   17680.0            NaN         patho  \n",
       "4    stage ib  alive    50.0   19106.0            NaN         patho  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chargement des données, clean_data étant les données nettoyées des NaN trop nombreux\n",
    "df_data = pd.read_csv(\"clean_data.csv\")\n",
    "print(\"df_data shape:\", df_data.shape)\n",
    "df_metadata = pd.read_csv(\"metadata.csv\")\n",
    "print(\"df_metadata shape:\", df_metadata.shape)\n",
    "df_metadata[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(685, 20103)\n",
      "labelsBinary : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0\n",
      " 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      " 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "valuesBinary : Index(['patho', 'normal'], dtype='object')\n",
      "labelsStages : [ 0  0  1  2  0  3  1  4  0  1  0  3  0  5  0  0  5  1  0  6  4  1  1 -1\n",
      " -1  0  1  1  1  0  7 -1  7  0  0  1  4  5  0  0  8  5  6  9  5  7  6 -1\n",
      "  5  3  1  1  1  5  7  5  3 -1 -1  0  5  0  4  7  0  0 -1  1 -1  1  5 -1\n",
      "  5  5  0  1  5 -1  0  3  1  7  3  5  0  5  3  5  0 -1  5  3  0  1  1  0\n",
      " -1  3  7  0  1 -1  5  2  0  5  4  1  0  9  0  3  0  1  0  3  5 -1 -1  5\n",
      " -1  3  1  3  5  9  1  4  0  1  1 -1  5  0  5  0  3  1  1 -1 -1  1  0  0\n",
      "  7  3  1  0  0  3  5  1  1  0  0  0  0  1  3  1  3  1  3  3  1  0  1  0\n",
      "  9 -1 -1  3  0  5  1  7  1  1  1  4  7  3  0  3  5  5  9  1  0  7 -1  1\n",
      "  1  0  3  7  6  1  5  5  5  5  3  0  0  1  1  7  1  1 -1  0  1  0  7  7\n",
      "  0  0  3  0  1  5  1  1  7  0  0  7  1  1  1  1  7  9  7  0  1 -1 -1  0\n",
      "  6  1  0  5  7 -1  5  3  5  3  3  0  1  5  0  5  0  0  3  7  3  5  1  0\n",
      "  3  1  1  4  1  5  0  3  4  1  1  3  1  1  3  0  0  1  1  7  0  5  1  1\n",
      "  5  5  1  1 -1  1  5  0  1  1  3  5  4 -1  3  5 -1  4  7  3  3  0  5  7\n",
      "  0  1  1  1 -1  0  3 -1  3 -1 -1  0  0  1  5  3  1  1  4  3  0  7  0  7\n",
      "  7  3  0 -1  4  1  9  0  3  5  0  1  0  7  0 -1  1 -1  1  0  1  5 -1  0\n",
      "  1  0  1  0  3  1  7  5  1 -1  0  0 -1  1  3  4  0  1  0  3  0  1  6 -1\n",
      "  5  0  1  1  0 -1 -1  3  1  7  4  1  4 -1  3  3  1  2 -1  7  7  5  0  0\n",
      "  1  3  2  3  7  4  5  3  0  7  5 -1  3  0  1  1  1  1  7  3  7  9  1  9\n",
      " -1 -1  5  3  5  5  3  1  1  5  0  1  0  7 -1  4  0  0  3  3  1  0  4  1\n",
      "  0  1  5  1  0  7  0  3  1  1 -1  7 -1  3  0  5  0  7  2  0  1  0  1  1\n",
      "  0  7  5  0  0  1  0  5  0  0  0  0  7  3  3  7  0  1  3  0  7  4 -1  3\n",
      "  1  5  3  1  3  3  3  4  7  1  0  4  3  0  5  6  0  0  0  1  0  0  4  0\n",
      "  7 -1  3 -1  0  0  3  5  5  1  1  1  3  5  1  5  6 -1  3  5  3  0  0  5\n",
      "  1  1  1  7  5  7  0  5  0  3  1  0  0  0  3 -1  1  1  0  1  4 -1  1  0\n",
      "  1  5  0  1  0  9  1  5  5  5  1  5  4 -1  0  7  3  5 -1  0 -1  0 -1  4\n",
      "  0  1  1  4  6  1  0  0  5  0  4  0  1  5  9  9 -1  3  1  0  0  3  5  0\n",
      "  4  0  3  4  3  0  1  3  0  5  1  0  0  6  0  1  0  1  0  1  1  1  5  0\n",
      "  1  1  5  3  0  1  7  0  0  5  5  5  1  3  1  0  7  7  0  0  7  0  0  0\n",
      "  3  4  3  0  5  3  1  0  9  3  9  3  9]\n",
      "valuesStages : Index(['stage ib', 'stage ia', 'stage i', 'stage iib', 'stage iv',\n",
      "       'stage iiia', 'not reported', 'stage iia', 'stage ii', 'stage iiib'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Convertir les données en ndarray et supprimer les colonnes inutiles\n",
    "D = df_data.loc[:, ~df_data.columns.str.contains('^Unnamed')].values\n",
    "D = D.T\n",
    "\n",
    "print(type(D))\n",
    "print(D.shape)\n",
    "\n",
    "# Générer les labels en fonction d'une colonne choisie\n",
    "status = pd.Series(df_metadata[\"tissue_status\"].values)\n",
    "stage = pd.Series(df_metadata[\"tumor_stage\"].values)\n",
    "\n",
    "labelsBinary, valuesBinary = pd.factorize(status)\n",
    "labelsStages, valuesStages = pd.factorize(stage)\n",
    "\n",
    "yBinary = labelsBinary\n",
    "yStage = labelsStages\n",
    "\n",
    "print(\"labelsBinary :\", labelsBinary)\n",
    "print(\"valuesBinary :\", valuesBinary)\n",
    "\n",
    "print(\"labelsStages :\", labelsStages)\n",
    "print(\"valuesStages :\", valuesStages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(685, 1000)\n"
     ]
    }
   ],
   "source": [
    "# IL FAUT ARRIVER À 300 MB après avoir agrandi le nombre d'observations\n",
    "# ===>>> couper les features\n",
    "# selection des k best features grâce au test chi2\n",
    "chi2_selector = SelectKBest(chi2, k=1000)\n",
    "D = chi2_selector.fit_transform(D, labelsBinary)\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 141, 1: 131, 5: 68, 3: 63, -1: 49, 7: 42, 4: 26, 9: 13, 6: 9, 2: 5, 8: 1})\n",
      "(547, 1000)\n",
      "(1000,)\n",
      "(548, 1000)\n",
      "[ 0  0  7  1  7  0  5  3  4 -1  0  0  5  0  7  0  6 -1  1  3  0  0  1  1\n",
      "  3 -1  1  5  1  5  0  3  0  4  7  7  3  3  0  0  1  0  0  0  4 -1 -1 -1\n",
      "  1  0  1  0 -1  0  9  6  5  0  0  0 -1  3  1  7  0  0  5  0  1  1  1  1\n",
      " -1  5  4  3  4  3 -1  0  7  0  5  1  1  6  1  9  1  0  7  5  1  1  0  5\n",
      "  7  0  5  0  3  0  3  0  0  6  7  0  1  1  5  3  7  0  1  5  5  1  1  7\n",
      "  0  1  0 -1 -1  0  1  0  0 -1  9  0 -1  0  3  1  0  7  5  1  3 -1  1  1\n",
      "  7  0  1  1  0  1  1  6  5  7  9  1  1  0  7  3  1  1  0  7  4  4  3  5\n",
      "  5  3  5 -1 -1  0  7  3  0  1  3  6  0  9  0  6  0  4  0  3 -1  1  7 -1\n",
      "  3  1  4  0  1  4  1  1  1  5  0 -1  9  1  0  1  1 -1  5  5  0 -1  3  4\n",
      "  0  1  7 -1  1  1  1  1  7  3  7  0  0  4  5  5  0  3  2  5  7 -1  1  7\n",
      "  0  5  1  0  1  1  0  1  3  1  5  1 -1  0  3  0  7  1  5  3  9 -1  0  5\n",
      "  1  1  5  1  9  9  3  3  0  0  1  1 -1 -1  5  1  1  1  0  0  0  2  0  0\n",
      "  6  0  3  0  5  5  5  1  7  1  7  7  7  5 -1  0  1  4 -1  5  5  4  5  1\n",
      "  0  3  5  5  1  1  1  1  5  5 -1  1  1 -1  0  0  0  4  1  0  3  5 -1  0\n",
      "  1  3  1  0 -1  0  5  0  1  0  3  1  9  3  0 -1  0  0  4  3  4  3  0  0\n",
      "  0  1  0  1  1  3  3  3  0  5  3  1  5  0  3  1  1 -1  5  0  0  3  0  4\n",
      "  7  1  1  3  4  5  0  0  7  5  1  9  1  0  2  4 -1  3  1  0  5  7  1  0\n",
      "  0  1  5  3  0  1  5  0  1  0  1  3 -1  5  1  1  0  6  0  1 -1  1  1 -1\n",
      "  5  0  0  0  3  7 -1  0 -1 -1  7  5  5  1  0  1  0  0  5  5  3  3  1  4\n",
      "  5  0  7  0  0  0  0  7  3  1  0  1  3  0  0  7  1  9  5  5  1  7  2  1\n",
      "  5  0 -1  0  1  3  1  1  0  3  3  4  7  3 -1  0  5  1  5  1  0  1  1  1\n",
      "  0  0  7  1  5  3  1  7  0  3 -1  2  0  1  1  1  1  3  1  1  0  4  0  0\n",
      "  3  3  1  0  0 -1  5  5  4  7 -1  3  9  4 -1  4  0  3  5]\n",
      "[ 0  0  7  1  7  0  5  3  4 -1  0  0  5  0  7  0  6 -1  1  3  0  0  1  1\n",
      "  3 -1  1  5  1  5  0  3  0  4  7  7  3  3  0  0  1  0  0  0  4 -1 -1 -1\n",
      "  1  0  1  0 -1  0  9  6  5  0  0  0 -1  3  1  7  0  0  5  0  1  1  1  1\n",
      " -1  5  4  3  4  3 -1  0  7  0  5  1  1  6  1  9  1  0  7  5  1  1  0  5\n",
      "  7  0  5  0  3  0  3  0  0  6  7  0  1  1  5  3  7  0  1  5  5  1  1  7\n",
      "  0  1  0 -1 -1  0  1  0  0 -1  9  0 -1  0  3  1  0  7  5  1  3 -1  1  1\n",
      "  7  0  1  1  0  1  1  6  5  7  9  1  1  0  7  3  1  1  0  7  4  4  3  5\n",
      "  5  3  5 -1 -1  0  7  3  0  1  3  6  0  9  0  6  0  4  0  3 -1  1  7 -1\n",
      "  3  1  4  0  1  4  1  1  1  5  0 -1  9  1  0  1  1 -1  5  5  0 -1  3  4\n",
      "  0  1  7 -1  1  1  1  1  7  3  7  0  0  4  5  5  0  3  2  5  7 -1  1  7\n",
      "  0  5  1  0  1  1  0  1  3  1  5  1 -1  0  3  0  7  1  5  3  9 -1  0  5\n",
      "  1  1  5  1  9  9  3  3  0  0  1  1 -1 -1  5  1  1  1  0  0  0  2  0  0\n",
      "  6  0  3  0  5  5  5  1  7  1  7  7  7  5 -1  0  1  4 -1  5  5  4  5  1\n",
      "  0  3  5  5  1  1  1  1  5  5 -1  1  1 -1  0  0  0  4  1  0  3  5 -1  0\n",
      "  1  3  1  0 -1  0  5  0  1  0  3  1  9  3  0 -1  0  0  4  3  4  3  0  0\n",
      "  0  1  0  1  1  3  3  3  0  5  3  1  5  0  3  1  1 -1  5  0  0  3  0  4\n",
      "  7  1  1  3  4  5  0  0  7  5  1  9  1  0  2  4 -1  3  1  0  5  7  1  0\n",
      "  0  1  5  3  0  1  5  0  1  0  1  3 -1  5  1  1  0  6  0  1 -1  1  1 -1\n",
      "  5  0  0  0  3  7 -1  0 -1 -1  7  5  5  1  0  1  0  0  5  5  3  3  1  4\n",
      "  5  0  7  0  0  0  0  7  3  1  0  1  3  0  0  7  1  9  5  5  1  7  2  1\n",
      "  5  0 -1  0  1  3  1  1  0  3  3  4  7  3 -1  0  5  1  5  1  0  1  1  1\n",
      "  0  0  7  1  5  3  1  7  0  3 -1  2  0  1  1  1  1  3  1  1  0  4  0  0\n",
      "  3  3  1  0  0 -1  5  5  4  7 -1  3  9  4 -1  4  0  3  5  2]\n"
     ]
    }
   ],
   "source": [
    "# 6 occurences nécessaire pour Smote ou Adasyn, \n",
    "# on enlève la classe 8 et on duplique une occurence de 2 pour passer à 6 (min SMOTE) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(D, yStage, test_size=0.2, random_state=42)\n",
    "# stratify=y, mais une classe avec 1 occurences,on vire cette classe?\n",
    "\n",
    "recounted = Counter(y_train)\n",
    "print(recounted)\n",
    "\n",
    "X_train_sans_8 = X_train[np.where(y_train!=8)]\n",
    "X_2 = X_train[np.where(y_train==2)][0]\n",
    "print(X_train_sans_8.shape)\n",
    "print(X_2.shape)\n",
    "\n",
    "# dédoublement d'un exemple de la classe 2\n",
    "X_train_sans_8_double_2 = np.vstack([X_train_sans_8,X_2])\n",
    "print(X_train_sans_8_double_2.shape)\n",
    "\n",
    "# dédoublement d'un label de la classe 2\n",
    "y_train_sans_8 = y_train[np.where(y_train!=8)]\n",
    "print(y_train_sans_8)\n",
    "y_train_sans_8 = np.append(y_train_sans_8, 2)\n",
    "print(y_train_sans_8)\n",
    "\n",
    "# retrait des instances potentielles de la classe 8 trop petite dans le test\n",
    "X_test = X_test[np.where(y_test!=8)]\n",
    "y_test = y_test[np.where(y_test!=8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 0 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 1 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 5 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 3 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class -1 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 7 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 4 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 9 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 6 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n",
      "/home/mbauw/anaconda3/lib/python3.7/site-packages/imblearn/utils/_validation.py:257: UserWarning: After over-sampling, the number of samples (1000) in class 2 will be larger than the number of samples in the majority class (class #0 -> 141)\n",
      "  n_samples_majority))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "(10000, 1000)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_sans_8_double_2\n",
    "y_train = y_train_sans_8\n",
    "\n",
    "# nombre d'occurences désirées par classe \n",
    "# # on a enlevé la classe 8 car qu'une occurence \n",
    "# impossible d'appliquer SMOTE ou ADASYN et génération d'une population à partir\n",
    "# d'un seul exemple est absurde\n",
    "\n",
    "dict= {0: 1000, 1: 1000, 5: 1000, 3: 1000, -1: 1000, 7: 1000, 4: 1000, 9: 1000, 6: 1000, 2: 1000}  \n",
    "smote = SMOTE(random_state=42, sampling_strategy=dict)\n",
    "\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "y_resampled = np.array([y_1 if y_1 != -1 else 8 for y_1 in y_resampled])\n",
    "print(set(y_resampled))\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage auto_ML global (TOUTES nos données): train, valid, test\n",
    "\n",
    "chaque ensemble (train, valid, test) doit être équilibré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1001)\n"
     ]
    }
   ],
   "source": [
    "# on récupère un data_frame de chaque classe, dans lesquelles on va piocher pour redéfinir chaque ensemble\n",
    "\n",
    "X_resampled_df = pd.DataFrame(X_resampled)\n",
    "y_resampled_df = pd.DataFrame(y_resampled)\n",
    "y_resampled_df = y_resampled_df.rename(columns={0: 'label'})\n",
    "\n",
    "X_and_y = pd.concat([X_resampled_df, y_resampled_df], axis=1, sort=False)\n",
    "#X_and_y.head(n=2)\n",
    "print(X_and_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_and_y['label'].values) # vérifie la disparition de la classe -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition d'un dataframe par classe pour découper par classe\n",
    "# et obtenir des train/valid/test équilibrés\n",
    "\n",
    "X_and_y_class0 = X_and_y[X_and_y[\"label\"] == 0]\n",
    "X_and_y_class1 = X_and_y[X_and_y[\"label\"] == 1]\n",
    "X_and_y_class2 = X_and_y[X_and_y[\"label\"] == 2]\n",
    "X_and_y_class3 = X_and_y[X_and_y[\"label\"] == 3]\n",
    "X_and_y_class4 = X_and_y[X_and_y[\"label\"] == 4]\n",
    "X_and_y_class5 = X_and_y[X_and_y[\"label\"] == 5]\n",
    "X_and_y_class6 = X_and_y[X_and_y[\"label\"] == 6]\n",
    "X_and_y_class7 = X_and_y[X_and_y[\"label\"] == 7]\n",
    "X_and_y_class8 = X_and_y[X_and_y[\"label\"] == 8]\n",
    "X_and_y_class9 = X_and_y[X_and_y[\"label\"] == 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN global de taille 800 (dans lequel on prendra tous les sets du starting kit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1001)\n",
      "(8000, 1001)\n"
     ]
    }
   ],
   "source": [
    "X_and_y_class0_train = X_and_y_class0[:800]\n",
    "print(X_and_y_class0_train.shape)\n",
    "\n",
    "X_and_y_class1_train = X_and_y_class1[:800]\n",
    "X_and_y_class2_train = X_and_y_class2[:800]\n",
    "X_and_y_class3_train = X_and_y_class3[:800]\n",
    "X_and_y_class4_train = X_and_y_class4[:800]\n",
    "X_and_y_class5_train = X_and_y_class5[:800]\n",
    "X_and_y_class6_train = X_and_y_class6[:800]\n",
    "X_and_y_class7_train = X_and_y_class7[:800]\n",
    "X_and_y_class8_train = X_and_y_class8[:800]\n",
    "X_and_y_class9_train = X_and_y_class9[:800]\n",
    "\n",
    "# TRAIN global par concaténations des 800 premiers de chaque classe pour obtenir un train équilibré\n",
    "X_and_y_train = pd.concat([X_and_y_class0_train,X_and_y_class1_train, X_and_y_class2_train,X_and_y_class3_train,X_and_y_class4_train,X_and_y_class5_train,X_and_y_class6_train,X_and_y_class7_train,X_and_y_class8_train,X_and_y_class9_train], axis=0, sort=False)\n",
    "print(X_and_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALID global de taille 100 (dans lequel on ne prendra RIEN pour le starting kit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1001)\n"
     ]
    }
   ],
   "source": [
    "X_and_y_class0_valid = X_and_y_class0[800:900]\n",
    "X_and_y_class1_valid = X_and_y_class1[800:900]\n",
    "X_and_y_class2_valid = X_and_y_class2[800:900]\n",
    "X_and_y_class3_valid = X_and_y_class3[800:900]\n",
    "X_and_y_class4_valid = X_and_y_class4[800:900]\n",
    "X_and_y_class5_valid = X_and_y_class5[800:900]\n",
    "X_and_y_class6_valid = X_and_y_class6[800:900]\n",
    "X_and_y_class7_valid = X_and_y_class7[800:900]\n",
    "X_and_y_class8_valid = X_and_y_class8[800:900]\n",
    "X_and_y_class9_valid = X_and_y_class9[800:900]\n",
    "\n",
    "# TRAIN global par concaténations des 800 premiers de chaque classe pour obtenir un train équilibré\n",
    "X_and_y_valid = pd.concat([X_and_y_class0_valid,X_and_y_class1_valid, X_and_y_class2_valid,X_and_y_class3_valid,X_and_y_class4_valid,X_and_y_class5_valid,X_and_y_class6_valid,X_and_y_class7_valid,X_and_y_class8_valid,X_and_y_class9_valid], axis=0, sort=False)\n",
    "print(X_and_y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST global de taille 100 (dans lequel on ne prendra RIEN pour le starting kit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage sample pour starting_kit dans le train précédent (sample train/valid/test provenant du train global !!!)\n",
    "\n",
    "\n",
    "ATTENTION CI DESSOUS COPIE COLLE NON ADAPTE (necessaire ?) de Sid Ali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../starting_kit/sample_data/hadaca_feat1.name', 'w') as f:\n",
    "    for i in range(X_resampled.shape[1]):\n",
    "        f.write('methyl_{}\\n'.format(i))\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_train.data', 'w') as f:\n",
    "    for x in X_resampled[50:100]:\n",
    "        for feat in x:\n",
    "            f.write('{} '.format(feat))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_test.data', 'w') as f:\n",
    "    for x in X_resampled[:50]:\n",
    "        for feat in x:\n",
    "            f.write('{} '.format(feat))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_valid.data', 'w') as f:\n",
    "    for x in X_resampled[100:150]:\n",
    "        for feat in x:\n",
    "            f.write('{} '.format(feat))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_train.solution', 'w') as f:\n",
    "    for x in y_resampled[50:100]:\n",
    "        f.write('{}'.format(x))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_test.solution', 'w') as f:\n",
    "    for x in y_resampled[:50]:\n",
    "        f.write('{}'.format(x))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_valid.solution', 'w') as f:\n",
    "    for x in y_resampled[100:150]:\n",
    "        f.write('{}'.format(x))\n",
    "\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
