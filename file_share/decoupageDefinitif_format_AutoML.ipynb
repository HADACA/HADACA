{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage définitif des données au format AutoML\n",
    "\n",
    "pour le format AutoML cf. la page https://github.com/madclam/m2aic2019/blob/master/Starting_Kit_M2info.pdf\n",
    "\n",
    "- extraire les données de Magali\n",
    "- les enrichir avec SMOTE\n",
    "- passer au format AutoML (train, valid, test), en découpant de manière à ce que les classes soient équilibrées à chaque fois\n",
    "\n",
    "### PLAN\n",
    "\n",
    "- 1) chargement des données clean et génération du dataset global (1000 de chaque classe avec SMOTE)\n",
    "- 2) découpage auto_ML global (TOUTES nos données): train, valid, test (800,100,100 pour chaque classe)\n",
    "- 3) découpage sample pour starting_kit dans le TRAIN (!!!) précédent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import ADASYN, SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données clean\n",
    "\n",
    "on charge les données et on utilise SMOTE pour créer l'ensemble \"global\" de données avec lequel on va travailler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/clean_data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c8e1764abae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Chargement des données, clean_data étant les données nettoyées des NaN trop nombreux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/clean_data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_data shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/metadata.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"df_metadata shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/clean_data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# Chargement des données, clean_data étant les données nettoyées des NaN trop nombreux\n",
    "df_data = pd.read_csv(\"data/clean_data.csv\")\n",
    "print(\"df_data shape:\", df_data.shape)\n",
    "df_metadata = pd.read_csv(\"data/metadata.csv\")\n",
    "print(\"df_metadata shape:\", df_metadata.shape)\n",
    "df_metadata[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les données en ndarray et supprimer les colonnes inutiles\n",
    "D = df_data.loc[:, ~df_data.columns.str.contains('^Unnamed')].values\n",
    "D = D.T\n",
    "\n",
    "print(type(D))\n",
    "print(D.shape)\n",
    "\n",
    "# Générer les labels en fonction d'une colonne choisie\n",
    "status = pd.Series(df_metadata[\"tissue_status\"].values)\n",
    "stage = pd.Series(df_metadata[\"tumor_stage\"].values)\n",
    "\n",
    "labelsBinary, valuesBinary = pd.factorize(status)\n",
    "labelsStages, valuesStages = pd.factorize(stage)\n",
    "\n",
    "yBinary = labelsBinary\n",
    "yStage = labelsStages\n",
    "\n",
    "print(\"labelsBinary :\", labelsBinary)\n",
    "print(\"valuesBinary :\", valuesBinary)\n",
    "\n",
    "print(\"labelsStages :\", labelsStages)\n",
    "print(\"valuesStages :\", valuesStages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IL FAUT ARRIVER À 300 MB après avoir agrandi le nombre d'observations\n",
    "# ===>>> couper les features\n",
    "# selection des k best features grâce au test chi2\n",
    "chi2_selector = SelectKBest(chi2, k=1000)\n",
    "D = chi2_selector.fit_transform(D, labelsBinary)\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 occurences nécessaire pour Smote ou Adasyn, \n",
    "# on enlève la classe 8 et on duplique une occurence de 2 pour passer à 6 (min nécessaire à SMOTE) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(D, yStage, test_size=0.2, random_state=42)\n",
    "# stratify=y, mais une classe avec 1 occurences,on vire cette classe?\n",
    "\n",
    "recounted = Counter(y_train)\n",
    "print(recounted)\n",
    "\n",
    "X_train_sans_8 = X_train[np.where(y_train!=8)]\n",
    "X_2 = X_train[np.where(y_train==2)][0]\n",
    "print(X_train_sans_8.shape)\n",
    "print(X_2.shape)\n",
    "\n",
    "# dédoublement d'un exemple de la classe 2\n",
    "X_train_sans_8_double_2 = np.vstack([X_train_sans_8,X_2])\n",
    "print(X_train_sans_8_double_2.shape)\n",
    "\n",
    "# dédoublement d'un label de la classe 2\n",
    "y_train_sans_8 = y_train[np.where(y_train!=8)]\n",
    "print(y_train_sans_8)\n",
    "y_train_sans_8 = np.append(y_train_sans_8, 2)\n",
    "print(y_train_sans_8)\n",
    "\n",
    "# retrait des instances potentielles de la classe 8 trop petite dans le test\n",
    "X_test = X_test[np.where(y_test!=8)]\n",
    "y_test = y_test[np.where(y_test!=8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_sans_8_double_2\n",
    "y_train = y_train_sans_8\n",
    "\n",
    "# nombre d'occurences désirées par classe \n",
    "# # on a enlevé la classe 8 car qu'une occurence \n",
    "# impossible d'appliquer SMOTE ou ADASYN et génération d'une population à partir\n",
    "# d'un seul exemple est absurde\n",
    "\n",
    "dict= {0: 1000, 1: 1000, 5: 1000, 3: 1000, -1: 1000, 7: 1000, 4: 1000, 9: 1000, 6: 1000, 2: 1000}  \n",
    "smote = SMOTE(random_state=42, sampling_strategy=dict)\n",
    "\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "y_resampled = np.array([y_1 if y_1 != -1 else 8 for y_1 in y_resampled])\n",
    "print(set(y_resampled))\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage auto_ML global (TOUTES nos données): train, valid, test\n",
    "\n",
    "chaque ensemble (train, valid, test) doit être équilibré\n",
    "\n",
    "La dernière colonne contient les labels pour X_and_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on récupère un data_frame de chaque classe, dans lesquelles on va piocher pour redéfinir chaque ensemble\n",
    "\n",
    "X_resampled_df = pd.DataFrame(X_resampled)\n",
    "y_resampled_df = pd.DataFrame(y_resampled)\n",
    "y_resampled_df = y_resampled_df.rename(columns={0: 'label'})\n",
    "\n",
    "\n",
    "# La dernière colonne contient les labels\n",
    "X_and_y = pd.concat([X_resampled_df, y_resampled_df], axis=1, sort=False)\n",
    "#X_and_y.head(n=2)\n",
    "print(X_and_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(X_and_y['label'].values) # vérifie la disparition de la classe -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition d'un dataframe par classe pour découper par classe\n",
    "# et obtenir des train/valid/test équilibrés\n",
    "\n",
    "X_and_y_class0 = X_and_y[X_and_y[\"label\"] == 0]\n",
    "X_and_y_class1 = X_and_y[X_and_y[\"label\"] == 1]\n",
    "X_and_y_class2 = X_and_y[X_and_y[\"label\"] == 2]\n",
    "X_and_y_class3 = X_and_y[X_and_y[\"label\"] == 3]\n",
    "X_and_y_class4 = X_and_y[X_and_y[\"label\"] == 4]\n",
    "X_and_y_class5 = X_and_y[X_and_y[\"label\"] == 5]\n",
    "X_and_y_class6 = X_and_y[X_and_y[\"label\"] == 6]\n",
    "X_and_y_class7 = X_and_y[X_and_y[\"label\"] == 7]\n",
    "X_and_y_class8 = X_and_y[X_and_y[\"label\"] == 8]\n",
    "X_and_y_class9 = X_and_y[X_and_y[\"label\"] == 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN global de taille 800 (dans lequel on prendra tous les sets du starting kit)\n",
    "\n",
    "La dernière colonne contient les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_and_y_class0_train = X_and_y_class0[:800]\n",
    "print(X_and_y_class0_train.shape)\n",
    "\n",
    "X_and_y_class1_train = X_and_y_class1[:800]\n",
    "X_and_y_class2_train = X_and_y_class2[:800]\n",
    "X_and_y_class3_train = X_and_y_class3[:800]\n",
    "X_and_y_class4_train = X_and_y_class4[:800]\n",
    "X_and_y_class5_train = X_and_y_class5[:800]\n",
    "X_and_y_class6_train = X_and_y_class6[:800]\n",
    "X_and_y_class7_train = X_and_y_class7[:800]\n",
    "X_and_y_class8_train = X_and_y_class8[:800]\n",
    "X_and_y_class9_train = X_and_y_class9[:800]\n",
    "\n",
    "# TRAIN global par concaténations des 800 premiers de chaque classe pour obtenir un train équilibré\n",
    "X_and_y_train = pd.concat([X_and_y_class0_train,X_and_y_class1_train, X_and_y_class2_train,X_and_y_class3_train,X_and_y_class4_train,X_and_y_class5_train,X_and_y_class6_train,X_and_y_class7_train,X_and_y_class8_train,X_and_y_class9_train], axis=0, sort=False)\n",
    "print(X_and_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALID global de taille 100 (dans lequel on ne prendra RIEN pour le starting kit)\n",
    "\n",
    "La dernière colonne contient les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_and_y_class0_valid = X_and_y_class0[800:900]\n",
    "X_and_y_class1_valid = X_and_y_class1[800:900]\n",
    "X_and_y_class2_valid = X_and_y_class2[800:900]\n",
    "X_and_y_class3_valid = X_and_y_class3[800:900]\n",
    "X_and_y_class4_valid = X_and_y_class4[800:900]\n",
    "X_and_y_class5_valid = X_and_y_class5[800:900]\n",
    "X_and_y_class6_valid = X_and_y_class6[800:900]\n",
    "X_and_y_class7_valid = X_and_y_class7[800:900]\n",
    "X_and_y_class8_valid = X_and_y_class8[800:900]\n",
    "X_and_y_class9_valid = X_and_y_class9[800:900]\n",
    "\n",
    "# TRAIN global par concaténations des 800 premiers de chaque classe pour obtenir un train équilibré\n",
    "X_and_y_valid = pd.concat([X_and_y_class0_valid,X_and_y_class1_valid, X_and_y_class2_valid,X_and_y_class3_valid,X_and_y_class4_valid,X_and_y_class5_valid,X_and_y_class6_valid,X_and_y_class7_valid,X_and_y_class8_valid,X_and_y_class9_valid], axis=0, sort=False)\n",
    "print(X_and_y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST global de taille 100 (dans lequel on ne prendra RIEN pour le starting kit)\n",
    "\n",
    "La dernière colonne contient les labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_and_y_class0_test = X_and_y_class0[800:900]\n",
    "X_and_y_class1_test = X_and_y_class1[800:900]\n",
    "X_and_y_class2_test = X_and_y_class2[800:900]\n",
    "X_and_y_class3_test = X_and_y_class3[800:900]\n",
    "X_and_y_class4_test = X_and_y_class4[800:900]\n",
    "X_and_y_class5_test = X_and_y_class5[800:900]\n",
    "X_and_y_class6_test = X_and_y_class6[800:900]\n",
    "X_and_y_class7_test = X_and_y_class7[800:900]\n",
    "X_and_y_class8_test = X_and_y_class8[800:900]\n",
    "X_and_y_class9_test = X_and_y_class9[800:900]\n",
    "\n",
    "# TRAIN global par concaténations des 800 premiers de chaque classe pour obtenir un train équilibré\n",
    "X_and_y_test = pd.concat([X_and_y_class0_test,X_and_y_class1_test, X_and_y_class2_test,X_and_y_class3_test,X_and_y_class4_test,X_and_y_class5_test,X_and_y_class6_test,X_and_y_class7_test,X_and_y_class8_test,X_and_y_class9_test], axis=0, sort=False)\n",
    "print(X_and_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Découpage sample pour starting_kit dans le train global défini dans les cellules précédentes (sample train/valid/test provenant TOUS du train global !!!)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Générer des sets *équilibrés* pour le starting kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 différents de chaque classe dans chacun des subsets du starting kit\n",
    "\n",
    "# TRAIN pour starting_kit\n",
    "X_and_y_class0_trainKit = X_and_y_class0[:5]\n",
    "X_and_y_class1_trainKit = X_and_y_class1[:5]\n",
    "X_and_y_class2_trainKit = X_and_y_class2[:5]\n",
    "X_and_y_class3_trainKit = X_and_y_class3[:5]\n",
    "X_and_y_class4_trainKit = X_and_y_class4[:5]\n",
    "X_and_y_class5_trainKit = X_and_y_class5[:5]\n",
    "X_and_y_class6_trainKit = X_and_y_class6[:5]\n",
    "X_and_y_class7_trainKit = X_and_y_class7[:5]\n",
    "X_and_y_class8_trainKit = X_and_y_class8[:5]\n",
    "X_and_y_class9_trainKit = X_and_y_class9[:5]\n",
    "\n",
    "X_and_y_trainKit = pd.concat([X_and_y_class0_trainKit,X_and_y_class1_trainKit,\\\n",
    "                              X_and_y_class2_trainKit,X_and_y_class3_trainKit,\\\n",
    "                              X_and_y_class4_trainKit,X_and_y_class5_trainKit,\\\n",
    "                              X_and_y_class6_trainKit,X_and_y_class7_trainKit,\\\n",
    "                              X_and_y_class8_trainKit,X_and_y_class9_trainKit,], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST pour starting_kit\n",
    "X_and_y_class0_testKit = X_and_y_class0[5:10]\n",
    "X_and_y_class1_testKit = X_and_y_class1[5:10]\n",
    "X_and_y_class2_testKit = X_and_y_class2[5:10]\n",
    "X_and_y_class3_testKit = X_and_y_class3[5:10]\n",
    "X_and_y_class4_testKit = X_and_y_class4[5:10]\n",
    "X_and_y_class5_testKit = X_and_y_class5[5:10]\n",
    "X_and_y_class6_testKit = X_and_y_class6[5:10]\n",
    "X_and_y_class7_testKit = X_and_y_class7[5:10]\n",
    "X_and_y_class8_testKit = X_and_y_class8[5:10]\n",
    "X_and_y_class9_testKit = X_and_y_class9[5:10]\n",
    "\n",
    "X_and_y_testKit = pd.concat([X_and_y_class0_testKit,X_and_y_class1_testKit,\\\n",
    "                              X_and_y_class2_testKit,X_and_y_class3_testKit,\\\n",
    "                              X_and_y_class4_testKit,X_and_y_class5_testKit,\\\n",
    "                              X_and_y_class6_testKit,X_and_y_class7_testKit,\\\n",
    "                              X_and_y_class8_testKit,X_and_y_class9_testKit,], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID pour starting_kit\n",
    "X_and_y_class0_validKit = X_and_y_class0[10:15]\n",
    "X_and_y_class1_validKit = X_and_y_class1[10:15]\n",
    "X_and_y_class2_validKit = X_and_y_class2[10:15]\n",
    "X_and_y_class3_validKit = X_and_y_class3[10:15]\n",
    "X_and_y_class4_validKit = X_and_y_class4[10:15]\n",
    "X_and_y_class5_validKit = X_and_y_class5[10:15]\n",
    "X_and_y_class6_validKit = X_and_y_class6[10:15]\n",
    "X_and_y_class7_validKit = X_and_y_class7[10:15]\n",
    "X_and_y_class8_validKit = X_and_y_class8[10:15]\n",
    "X_and_y_class9_validKit = X_and_y_class9[10:15]\n",
    "\n",
    "X_and_y_validKit = pd.concat([X_and_y_class0_validKit,X_and_y_class1_validKit,\\\n",
    "                              X_and_y_class2_validKit,X_and_y_class3_validKit,\\\n",
    "                              X_and_y_class4_validKit,X_and_y_class5_validKit,\\\n",
    "                              X_and_y_class6_validKit,X_and_y_class7_validKit,\\\n",
    "                              X_and_y_class8_validKit,X_and_y_class9_validKit,], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Il faut re-séparer les features des labels (contenus dans la dernière colonne des X_and_y_...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_startingKit = X_and_y_train.iloc[:,:-1] # pour l'écriture du .name\n",
    "\n",
    "X_trainKit = X_and_y_trainKit.iloc[:,:-1]\n",
    "X_testKit = X_and_y_testKit.iloc[:,:-1]\n",
    "X_validKit = X_and_y_validKit.iloc[:,:-1]\n",
    "\n",
    "Y_trainKit = X_and_y_trainKit.iloc[:,-1]\n",
    "Y_testKit = X_and_y_testKit.iloc[:,-1]\n",
    "Y_validKit = X_and_y_validKit.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainKit.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_trainKit.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) puis écrire dans les fichiers AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../starting_kit/sample_data/hadaca_feat.name', 'w') as f:\n",
    "    for i in range(X_trainKit.values.shape[1]):\n",
    "        f.write('methyl_{}\\n'.format(i))\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_train.data', 'w') as f:\n",
    "    for x in X_trainKit.values:\n",
    "        for feat in x:\n",
    "            f.write('{} '.format(np.float64(feat)))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_test.data', 'w') as f:\n",
    "    for x in X_testKit.values:\n",
    "        for feat in x:\n",
    "            f.write('{} '.format(np.float64(feat)))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_valid.data', 'w') as f:\n",
    "    for x in X_validKit.values:\n",
    "        for feat in x:\n",
    "            f.write('{} '.format(np.float64(feat)))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_train.solution', 'w') as f:\n",
    "    for x in Y_trainKit.values:\n",
    "        f.write('{}'.format(x))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_test.solution', 'w') as f:\n",
    "    for x in Y_testKit.values:\n",
    "        f.write('{}'.format(x))\n",
    "\n",
    "        f.write('\\n')\n",
    "\n",
    "\n",
    "with open('../starting_kit/sample_data/hadaca_valid.solution', 'w') as f:\n",
    "    for x in Y_validKit.values:\n",
    "        f.write('{}'.format(x))\n",
    "\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
